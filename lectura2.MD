The main focus of this paper are matrix factorization techniques for recommender systems. It begins by giving a general introduction to recommender systems and it's motivations, then explains what content filtering is as well as collaborative filtering and describes it's primary areas: *neighborhood methods* and *latent factor models*. Then with all those preliminaries covered, the paper digs deeper into **matrix factorization techniques** starting with a basic model and two learning algorithms. After that, more complex models that allow for capturing certain characteristics like a user and item bias, temporal dynamics and confidence levels are developed. Finally, the authors share their experience with this model in the Netflix Prize competition.

I think this work was very easy to read. It explains the context of the recommender systems and some existing approaches in a clear way. Part of the easiness to read is explained by the simplicity of the matrix factorization techniques. However, I think that the section about temporal dynamics is a little to general, it is not clear how such system should be trained, should we create bins in the temporal space and train separated models for each bin? Let this not be misunderstood, I think that the value of adding a temporal component is relevant and the results in the paper clearly show it, but as I said it's not completely clear how to use it. The lack of development in this section may be because at the time the Netfix Prize was still running.

An interesing charactersitic of these methods is the fact that the resulting representation vectors for items and users can be interpreted in a specific way. We can identify dimensions in the learnt representations for particular things in the items that the user can be base their decisions, which might be quite usefull not only to make recommendations but also for understanding user tastes. This representations can then be used to create clusters of users based on their tastes.

A concerning thing with thiese methods is how fast they learn. The paper gives two learning algorithms and claims that one is more parallelizable than the other, but does not mention anything about how hard is to train a model or the computational cost of it. I think it is an important aspect to be aware given that some of the presented models have several learnable parameters. On the same line, does adding the confidence score parameter slow down training time? I mention it because assuming the $c_{ui}$ parameter is within $[0, 1]$, having bigger error in the predictions will decrease the value of $c_{ui}$ and it in turn will decrease the gradient of the errors which would yield a slower learning.

As I said it is a very well written paper, easy to understand and impressing given the simplicity of the models, however it lacks on technicals aspects.

